---
layout:     post
title:      自编码与自回归
subtitle:   AutoEncoding与AutoRegressive
date:       2020-06-16
author:     矫红岩
header-img: img/go-bg.png
catalog: true
tags:
    - NLP
---
## 自编码AE
通常的模型结构为：
input -> encoder -> hidden -> decoder -> output
自编码模型，输入与输出相同。目的是为了训练参数得到更好的hidden。
可以把Bert看成一种AutoEncoder，它通过Mask改变了部分Token，然后试图通过其上下文的其它Token来恢复这些被Mask的Token。
## 自回归AR
在时间序列上单向计算，GPT与ELMo都是自回归模型。

